# -*- coding: utf-8 -*-
"""PFL- Data Extraction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1bpHd7QDz4ADq26oHMSpJ3oMz4cVeB1vh
"""

import pandas as pd
import time
import requests
import numpy as np

Base_url="https://fbref.com/en/comps/9/2023-2024/stats/2023-2024-Premier-League-Stats"

season_data=pd.DataFrame()

"""**Data Extraction:**

pd.read_html() reads tables top-down and returns the first <table> it finds, which may not be the player stats table. On FBref, the correct table is often hidden in HTML comments.
"""

tables = pd.read_html(Base_url)   # Returns a list of all tables on the page

df_1 = tables[1]
df_2=tables[0]               # Select the first table (or pick the one you want)
print(df_1.head(1))

len(tables)

df_1

df_2

"""No player stats standards table, we need to find a new way

First Attempt - Clean_HTML
"""

import requests
import pandas as pd
from io import StringIO

url = "https://fbref.com/en/comps/9/2023-2024/stats/2023-2024-Premier-League-Stats"

response = requests.get(url)
html = response.text

# Remove HTML comment wrappers
html_clean = html.replace("<!--", "").replace("-->", "")

# Wrap in StringIO to satisfy pandas.read_html
html_io = StringIO(html_clean)
tables=pd.read_html(html_io)

"""2nd Attempt - try to mimic a browser

"""

import requests
import pandas as pd
from io import StringIO
from time import sleep

# List of last 5 seasons and their FBref URLs
season_urls = {
    "2019-2020": "https://fbref.com/en/comps/9/2019-2020/stats/2019-2020-Premier-League-Stats",
    "2020-2021": "https://fbref.com/en/comps/9/2020-2021/stats/2020-2021-Premier-League-Stats",
    "2021-2022": "https://fbref.com/en/comps/9/2021-2022/stats/2021-2022-Premier-League-Stats",
    "2022-2023": "https://fbref.com/en/comps/9/2022-2023/stats/2022-2023-Premier-League-Stats",
    "2023-2024": "https://fbref.com/en/comps/9/2023-2024/stats/2023-2024-Premier-League-Stats"
}

all_seasons = []

for season, url in season_urls.items():
    print(f"Scraping {season}...")

    headers = {"User-Agent": "Mozilla/5.0"}
    response = requests.get(url, headers=headers)

    if response.status_code != 200:
        print(f"Failed to load {season} (Status {response.status_code})")
        continue

    html = response.text.replace("<!--", "").replace("-->", "")
    html_io = StringIO(html)

    # Read all tables
    tables = pd.read_html(html_io)

    # Usually the Player Standard Stats table is the second table (tables[2])
    try:
        season_df = tables[2].copy()
        season_df["Season"] = season
        all_seasons.append(season_df)
        print(f"Collected {len(season_df)} rows for {season}")
    except IndexError:
        print(f"Player stats table not found for {season}")

    sleep(1)  # polite delay

    player_stats_last5 = pd.concat(all_seasons, ignore_index=True)
    player_stats_last5.to_csv("player_standard_stats_last5.csv", index=False)

import requests
import pandas as pd
import re
from io import StringIO
from time import sleep

headers = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
}

# ‚úÖ Correct URLs for last 5 seasons (FBref link structure)
season_urls = {
    "2023-2024": "https://fbref.com/en/comps/9/stats/Premier-League-Stats",
    "2022-2023": "https://fbref.com/en/comps/9/2022-2023/stats/2022-2023-Premier-League-Stats",
    "2021-2022": "https://fbref.com/en/comps/9/2021-2022/stats/2021-2022-Premier-League-Stats",
    "2020-2021": "https://fbref.com/en/comps/9/2020-2021/stats/2020-2021-Premier-League-Stats",
    "2019-2020": "https://fbref.com/en/comps/9/2019-2020/stats/2019-2020-Premier-League-Stats"
}

all_stats = []

for season, url in season_urls.items():
    print(f"\nüîÑ Scraping {season} - {url}")

    response = requests.get(url, headers=headers)

    if response.status_code != 200:
        print(f"‚ùå Failed to load {season} (Status {response.status_code})")
        continue

    html = response.text.replace("<!--", "").replace("-->", "")

    # Find Player Standard Stats table
    table = re.search(r'(<table.*?id="stats_standard".*?</table>)', html, re.DOTALL)
    if not table:
        print(f"‚ùå Player Standard Stats table not found for {season}")
        continue

    table_html = table.group(1)

    # Read into pandas
    df = pd.read_html(StringIO(table_html))[0]
    df = df[df["Rk"] != "Rk"]   # Remove repeating headers
    df["Season"] = season       # Add season column

    all_stats.append(df)
    print(f"‚úÖ Collected {len(df)} players for {season}")
    sleep(1)

# Combine and save
if all_stats:
    final_df = pd.concat(all_stats, ignore_index=True)
    final_df.to_csv("player_standard_stats_last5.csv", index=False)
    print("\nüéâ Done! Saved as player_standard_stats_last5.csv")
else:
    print("\n‚ö† No data collected.")

!pip install selenium pandas webdriver-manager

import cloudscraper
import pandas as pd
import re
from io import StringIO

# Create scraper session
scraper = cloudscraper.create_scraper()

url = "https://fbref.com/en/comps/9/2023-2024/stats/2023-2024-Premier-League-Stats"

# Get page content
response = scraper.get(url)
html = response.text.replace("<!--", "").replace("-->", "")  # remove commented-out table tags

# Find the stats table (standard player stats)
table = re.search(r'(<table.*?id="stats_standard".*?</table>)', html, re.DOTALL)

if table:
    df = pd.read_html(StringIO(table.group(1)), header=[0,1])[0]  # read multi-level header

    # ‚úÖ Flatten column MultiIndex: ('Player', 'Player') ‚Üí 'Player', ('Performance', 'Gls') ‚Üí 'Performance_Gls'
    df.columns = ['_'.join(col).strip() if isinstance(col, tuple) else col for col in df.columns]

    # ‚úÖ Detect any ranking column ('Rk', 'Rk_', 'Unnamed: 0_level_0', etc.)
    rk_col = [col for col in df.columns if col.lower().startswith('rk') or 'rank' in col.lower()]

    if rk_col:
        rk_col = rk_col[0]
        # Remove rows where header repeats (e.g., 'Rk' in data)
        df = df[df[rk_col] != rk_col]

    # Reset index after cleaning
    df = df.reset_index(drop=True)

    print(df.head())
else:
    print("‚ùå Stats table not found!")

df

"""Let me explain:

* Our table is hidden in HTML so the normal way wouldn't be sucsseful.
* Our table use multi-indexing.
* The website blocks our request using cloudfair.

What we had to do:
* Clean the HTML to get our table.
* Clean the data (Flatening)
* Use Cloudscraper to bypass the limitations
"""



import cloudscraper
import pandas as pd
import re
from io import StringIO

scraper = cloudscraper.create_scraper()
url = "https://fbref.com/en/comps/9/2023-2024/stats/2023-2024-Premier-League-Stats"

response = scraper.get(url)
html = response.text.replace("<!--", "").replace("-->", "")

# Find the Standard Stats table
table = re.search(r'(<table.*?id="stats_standard".*?</table>)', html, re.DOTALL)

if table:
    # Read MultiIndex table
    df = pd.read_html(StringIO(table.group(1)), header=[0,1])[2]

    # Flatten MultiIndex
    df.columns = ['_'.join([str(c) for c in col]).strip() if isinstance(col, tuple) else col for col in df.columns]

    # Detect the ranking column: first column containing 'Rk' or numeric values
    rank_col_candidates = [c for c in df.columns if 'rk' in c.lower()]
    if rank_col_candidates:
        rank_col = rank_col_candidates[0]
    else:
        # fallback: first column
        rank_col = df.columns[0]

    # Remove repeated header rows
    df = df[df[rank_col] != 'Rk']

    # Reset index
    df = df.reset_index(drop=True)

    # Rename Rk column
    df = df.rename(columns={rank_col: 'Rk'})

    print(f"Shape: {df.shape}")
    print(df.head())
else:
    print("‚ùå Standard Stats table not found!")

import cloudscraper
import pandas as pd
import re
from io import StringIO
from time import sleep

# List of season URLs
season_urls = {
    "2024-2025": "https://fbref.com/en/comps/9/2024-2025/stats/2024-2025-Premier-League-Stats",  # future URL, may not exist yet
    "2023-2024": "https://fbref.com/en/comps/9/2023-2024/stats/2023-2024-Premier-League-Stats",
    "2022-2023": "https://fbref.com/en/comps/9/2022-2023/stats/2022-2023-Premier-League-Stats",
    "2021-2022": "https://fbref.com/en/comps/9/2021-2022/stats/2021-2022-Premier-League-Stats",
    "2020-2021": "https://fbref.com/en/comps/9/2020-2021/stats/2020-2021-Premier-League-Stats"
}

all_seasons = []

scraper = cloudscraper.create_scraper()

for season, url in season_urls.items():
    print(f"üîÑ Scraping {season} ...")
    response = scraper.get(url)

    if response.status_code != 200:
        print(f"‚ùå Failed to load {season} (Status {response.status_code})")
        continue

    html = response.text.replace("<!--", "").replace("-->", "")

    # Find the standard stats table
    table = re.search(r'(<table.*?id="stats_standard".*?</table>)', html, re.DOTALL)
    if not table:
        print(f"‚ùå Table not found for {season}")
        continue

    # Read table with multi-level header
    df = pd.read_html(StringIO(table.group(1)), header=[0,1])[2]

    # Flatten columns
    df.columns = ['_'.join([str(c) for c in col]).strip() if isinstance(col, tuple) else col for col in df.columns]

    # Detect Rk column
    rank_col_candidates = [c for c in df.columns if 'rk' in c.lower()]
    if rank_col_candidates:
        rank_col = rank_col_candidates[0]
    else:
        rank_col = df.columns[0]  # fallback

    # Remove repeated headers
    df = df[df[rank_col] != 'Rk']

    # Reset index
    df = df.reset_index(drop=True)

    # Rename Rk column
    df = df.rename(columns={rank_col: 'Rk'})

    # Add season column
    df['Season'] = season

    print(f"‚úÖ Collected {len(df)} players for {season}")

    all_seasons.append(df)
    sleep(1)  # polite delay

# Combine all seasons
if all_seasons:
    final_df = pd.concat(all_seasons, ignore_index=True)
    print(f"\nüéâ Combined dataset shape: {final_df.shape}")
    # Save to CSV
    final_df.to_csv("Player_Standard_Stats_Last5Seasons.csv", index=False)
    print("‚úÖ Saved to Player_Standard_Stats_Last5Seasons.csv")
else:
    print("‚ö† No data collected!")

"""**Data Inspection and cleansing**"""

player_stats=final_df.copy()

print(player_stats.dtypes)

# Show count of NaNs per column
nan_counts = player_stats.isna().sum()
print(nan_counts.sort_values(ascending=False))

# Quick info about columns and data types
print(player_stats.info())

# View first few rows
print(player_stats.head())

# Numeric summary
print(player_stats.describe())

# For categorical columns
print(player_stats['Unnamed: 3_level_0_Pos'].value_counts())
print(player_stats['Unnamed: 4_level_0_Squad'].value_counts())

import seaborn as sns
import matplotlib.pyplot as plt

# --- 2Ô∏è‚É£ Inspect actual columns ---
print("Columns in the DataFrame:")
print(player_stats.columns.tolist())

# Copy the DataFrame
player_stats_clean = player_stats.copy()

# Rename Unnamed columns using their second level (after the last underscore)
new_columns = []
for col in player_stats_clean.columns:
    if col.startswith("Unnamed"):
        # take the part after the last underscore
        new_columns.append(col.split("_")[-1])
    else:
        new_columns.append(col)

player_stats_clean.columns = new_columns

# Drop the 'Matches' column if it exists
if 'Matches' in player_stats_clean.columns:
    player_stats_clean = player_stats_clean.drop(columns=['Matches'])

# Check result
print(player_stats_clean.columns.tolist())

player_stats_clean.to_csv("player_stats5_final.csv")

"""

**Data Clustering for 2023-2024 (Test)**"""

import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA

# Copy the cleaned DataFrame
df = player_stats_clean.copy()

# --- 1Ô∏è‚É£ Remove any repeating header rows or non-numeric rows ---
df = df[pd.to_numeric(df['Performance_Gls'], errors='coerce').notnull()]

# --- 2Ô∏è‚É£ Identify numeric columns for clustering ---
numeric_cols = [
    'Playing Time_MP', 'Playing Time_Starts', 'Playing Time_Min', 'Playing Time_90s',
    'Performance_Gls', 'Performance_Ast', 'Performance_G+A', 'Performance_G-PK',
    'Performance_PK', 'Performance_PKatt', 'Performance_CrdY', 'Performance_CrdR',
    'Expected_xG', 'Expected_npxG', 'Expected_xAG', 'Expected_npxG+xAG',
    'Progression_PrgC', 'Progression_PrgP', 'Progression_PrgR',
    'Per 90 Minutes_Gls', 'Per 90 Minutes_Ast', 'Per 90 Minutes_G+A',
    'Per 90 Minutes_G-PK', 'Per 90 Minutes_G+A-PK', 'Per 90 Minutes_xG',
    'Per 90 Minutes_xAG', 'Per 90 Minutes_xG+xAG', 'Per 90 Minutes_npxG',
    'Per 90 Minutes_npxG+xAG'
]

# Convert to numeric and fill NaNs
for col in numeric_cols:
    df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0)

# --- 3Ô∏è‚É£ Scale numeric data ---
scaler = StandardScaler()
X_scaled = scaler.fit_transform(df[numeric_cols])
# --- 4Ô∏è‚É£ Determine optimal number of clusters (elbow method) ---
inertia = []
for k in range(2, 11):
    km = KMeans(n_clusters=k, random_state=42)
    km.fit(X_scaled)
    inertia.append(km.inertia_)

plt.figure(figsize=(8,4))
plt.plot(range(2,11), inertia, marker='o')
plt.xlabel("Number of clusters")
plt.ylabel("Inertia")
plt.title("Elbow Method for KMeans")
plt.show()

k = 3  # Example, pick from elbow plot
kmeans = KMeans(n_clusters=k, random_state=42)
df['Cluster'] = kmeans.fit_predict(X_scaled)

# --- 6Ô∏è‚É£ Inspect clusters ---
print(df[['Player', 'Pos', 'Squad', 'Cluster']].head())

# Optional: visualize clusters using first 2 principal components
from sklearn.decomposition import PCA

pca = PCA(n_components=2)
components = pca.fit_transform(X_scaled)
df['PC1'] = components[:,0]
df['PC2'] = components[:,1]

export_cols = ['Player', 'Pos', 'Squad', 'Cluster', 'PC1', 'PC2'] + numeric_cols
export_df = df[export_cols].copy()
export_df.to_csv("player_stats_clustered_powerbi.csv", index=False)

print("‚úÖ Clustering done and CSV exported!")
print(export_df.head())


plt.figure(figsize=(8,6))
sns.scatterplot(data=df, x='PC1', y='PC2', hue='Cluster', palette='Set2')
plt.title("Player Clusters (PCA 2D Projection)")
plt.show()

